{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web_crawling6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1CxJ-p6D2YH_JN2aHGA5s5NebJVDnMMwu",
      "authorship_tag": "ABX9TyPAK84/2VjT40ziZls4E3Th",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soominimini/news_crawling/blob/main/using_keyword_crawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjnAOPuoAKZW",
        "outputId": "f36722fc-4718-4f85-8013-ca77f0269c9f"
      },
      "source": [
        "!pip install feedparser"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting feedparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 20.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 11.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 71kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.1MB/s \n",
            "\u001b[?25hCollecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp36-none-any.whl size=6067 sha256=80f7362f739d8c79fd72ccf766e7b8bcca70ad00a885d4fe17540ba085a75dde\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.2 sgmllib3k-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGo0BjpoAc4M",
        "outputId": "ec3a76c1-61a8-401b-bedf-1bff8dc78e32"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/21/9e2c0dbf9df856e6392a1aec1d18006c60b175aa4e31d351e8278a8a63c0/JPype1-1.2.0-cp36-cp36m-manylinux2010_x86_64.whl (453kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 29.7MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: tweepy, JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zs0f2u0koj1"
      },
      "source": [
        "import time\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import feedparser\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib\n",
        "import urllib.request as req\n",
        "import requests\n",
        "from konlpy.tag import Kkma, Okt, Komoran\n",
        "\n",
        "okt = Okt()\n",
        "from konlpy.utils import pprint\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0VKZ02oALEx"
      },
      "source": [
        "\n",
        "\n",
        "href_list = []  # 기사 주소가 들어갈 리스트\n",
        "TitDesc_list = []  # 제목 + 요약 내용 리스트\n",
        "title_list = []  # 제목 리스트\n",
        "\n",
        "\n",
        "def Crawling(keyword, start, end, pageNum):\n",
        "    keyword = '+'.join(keyword.split(' '))\n",
        "\n",
        "    last = False\n",
        "    page_num = 1\n",
        "\n",
        "    ds = start\n",
        "    de = end\n",
        "    while page_num != pageNum:\n",
        "        url = \"https://search.naver.com/search.naver?&where=news&query={0}&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={1}&de={2}&docid=&nso=so:r,p:,a:all&mynews=1&cluster_rank=238&start={3}&refresh_start=0\".format(\n",
        "            keyword, ds, de, str(page_num))\n",
        "        raw = requests.get(url)\n",
        "        # print(\"raw : \",raw)\n",
        "        html = raw.content\n",
        "        # print(html)\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        ul = soup.find('ul', {'class':'list_news'})\n",
        "\n",
        "        # ul = soup.find('ul', {'class': 'type01'})\n",
        "        li_list = ul.findAll('li')\n",
        "        for li in li_list:\n",
        "            try:\n",
        "              # soup.select_one('p.youngone#junu')\n",
        "                new = li.select_one('div.news_area')\n",
        "                # print(new)\n",
        "\n",
        "                new_title = new.select_one('a.news_tit')\n",
        "                naver_news_link = new.select_one('div.news_info')\n",
        "                # print(naver_news_link)\n",
        "                \n",
        "# 예시) soup.select('p')[1] 를 입력 후 실행하면, 여러 개의 p태그 중에서 두 번째 p태그의 내용만 나옵니다.\n",
        "                naver_news_url = li.select('a')[2]\n",
        "                # print(\"li.select('a')[2] : \",li.select('a')[2])\n",
        "\n",
        "\n",
        "                title = new_title['title']\n",
        "                link = naver_news_url['href']\n",
        "                # print(link)\n",
        "                if(keyword in title and \"네이버뉴스\" in naver_news_url ):\n",
        "                  link = link.replace('amp;', '')\n",
        "                  # print(link)\n",
        "                  href_list.append(link)\n",
        "                  \n",
        "\n",
        "\n",
        "            except AttributeError:\n",
        "                pass\n",
        "\n",
        "        # 마지막 페이지 주소 확인 (다음페이지 버튼이 없으면 종료페이지로 간주)\n",
        "        page = soup.find('div', {'class': 'sc_page'})\n",
        "        page_a_list = page.findAll('a')\n",
        "        if '다음' in str(page_a_list[-1]):\n",
        "            page_num += 1\n",
        "        else:\n",
        "            last = True\n",
        "\n",
        "\n",
        "Crawling('KR모터스', '2018.01.01', '2018.12.31', 50)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX9hL5jv_FHF",
        "outputId": "8803591a-6f2f-494f-e6d4-8a7fd6b61d16"
      },
      "source": [
        "# print(href_list)\n",
        "# duplicated url eliminate\n",
        "href_list = set(href_list)\n",
        "print(\"after set : \",href_list)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "after set :  {'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=011&aid=0003195661', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=005&aid=0001149146', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=015&aid=0004001316', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=018&aid=0004048723', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=277&aid=0004326264', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=015&aid=0004010903', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=015&aid=0003975697', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=005&aid=0001160286', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=014&aid=0004054182', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=030&aid=0002740508', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=018&aid=0004062967', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=015&aid=0004023273', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=004&oid=215&aid=0000591635', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=030&aid=0002771620', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=015&aid=0004029212', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=015&aid=0004017674', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=277&aid=0004378402', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=015&aid=0004064371', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=009&aid=0004267614', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=014&aid=0003943677', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=030&aid=0002718501', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=015&aid=0004004144'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4g-_FlFAfhp"
      },
      "source": [
        "def write_news_article_urls(output_file, urls):\n",
        "    \"\"\"\n",
        "    기사 URL들을 출력 파일에 기록한다.\n",
        "    :param output_file:\n",
        "    :param urls:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    for url in urls:\n",
        "        print(url, file=output_file)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0e9nKuaAa7g"
      },
      "source": [
        "def ext_news_article_urls(html):\n",
        "    \"\"\"\n",
        "    주어진 html에서 기사 url을 추출하여 돌려준다.\n",
        "    :param html:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    url_frags = re.findall('<a href=\"(.*?)\"', html)\n",
        "    news_article_urls=[]\n",
        "\n",
        "    for url_frag in url_frags:\n",
        "        if  \"sid1=101&sid2=258\" in url_frag and \"aid\" in url_frag:\n",
        "            news_article_urls.append(url_frag)\n",
        "        else :\n",
        "            continue\n",
        "\n",
        "    return news_article_urls\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbKaoTWtk53b"
      },
      "source": [
        "def write_html(output_file, html):\n",
        "    \"\"\"\n",
        "    주어진 HTML 텍스트를 출력 파일에 쓴다.\n",
        "    :param output_file:\n",
        "    :param html:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    output_file.write(\"{}\\n\".format(html))\n",
        "    output_file.write(\"@@@@@ ARTICLE DELMITER @@@@\\n\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ReXt-HLk8dH"
      },
      "source": [
        "\n",
        "target_word = \"KR모터스\" #input(\"Enter keyword : \")\n",
        "target_date = \"20180101\"\n",
        "output_file_name =  \"/content/drive/MyDrive/Colab Notebooks/stock_ML/html\"+target_word+\"__\"+target_date+\".txt\"#input(\"Enter output file name : \")\n",
        "\n",
        "output_file = open(output_file_name, \"w\", encoding='utf-8')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2G04QqnmOIY"
      },
      "source": [
        "\n",
        "for line in href_list:\n",
        "\n",
        "    article_id = line[(len(line) - 24):len(line)]\n",
        "    print_url = \"http://news.naver.com/main/tool/print.nhn?\" + article_id\n",
        "\n",
        "    # article_id = line[(len(line) - 24):len(line)]\n",
        "    # print_url = line\n",
        "\n",
        "\n",
        "    user_agent = \"'Mozilla/5.0\"\n",
        "    headers = {\"User-Agent\": user_agent}\n",
        "    try:\n",
        "      # print(\"Success!\")\n",
        "      response = requests.get(print_url, headers=headers)\n",
        "      html = response.text\n",
        "      write_html(output_file, html)\n",
        "\n",
        "    except requests.exceptions.RequestException as error:\n",
        "      print(\"Error: \", error)\n",
        "\n",
        "output_file.close()\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSffssNmmw2l"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "네이버 뉴스 기사 HTML에서 순수 텍스트 기사를 추출한다.\n",
        "\"\"\"\n",
        "\n",
        "import bs4\n",
        "import time\n",
        "import re\n",
        "import requests\n",
        "import os\n",
        "\n",
        "\n",
        "ARTICLE_DELIMITER = \"@@@@@ ARTICLE DELMITER @@@@\\n\"\n",
        "TITLE_START_PAT = '<h3 class=\"font1\">'\n",
        "TITLE_END_PAT = '</h3>'\n",
        "DATE_TIME_START_PAT = u'기사입력 <span class=\"t11\">'\n",
        "BODY_START_PAT = '<div class=\"article_body\">'\n",
        "BODY_END_PAT = '<div class=\"article_footer\">'\n",
        "TIDYUP_START_PAT = '<div class=\"article_footer\">'\n",
        "\n",
        "\n",
        "def read_html_article(html_file):\n",
        "    \"\"\"\n",
        "    HTML 파일에서 기사 하나를 읽어서 돌려준다.\n",
        "    :param html_file:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    lines = []\n",
        "    for line in html_file:\n",
        "        if line.startswith(ARTICLE_DELIMITER):\n",
        "            html_text = \"\".join(lines).strip()\n",
        "            return html_text\n",
        "        lines.append(line)\n",
        "\n",
        "    return None\n",
        "\n",
        "def ext_title(html_text):\n",
        "    \"\"\"\n",
        "    HTML 기사에서 제목을 추출하여 돌려준다.\n",
        "    :param html_text:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    p = html_text.find(TITLE_START_PAT)\n",
        "    q = html_text.find(TITLE_END_PAT)\n",
        "    title = html_text[p + len(TITLE_START_PAT):q]\n",
        "    title = title.strip()\n",
        "\n",
        "    return title\n",
        "\n",
        "\n",
        "def ext_date_time(html_text):\n",
        "    \"\"\"\n",
        "    HTML 기사에서 날짜와 시간을 추출하여 돌려준다.\n",
        "    :param html_text:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    start_p = html_text.find(DATE_TIME_START_PAT)+len(DATE_TIME_START_PAT)\n",
        "    end_p = start_p + 10\n",
        "    date_time = html_text[start_p:end_p]\n",
        "    date_time = date_time.strip()\n",
        "\n",
        "    return date_time\n",
        "\n",
        "def strip_html(html_body):\n",
        "    \"\"\"\n",
        "    HTML 본문에서 HTML 태그를 제거하고 돌려준다.\n",
        "    :param html_body:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    page = bs4.BeautifulSoup(html_body, \"html.parser\")\n",
        "    body = page.text\n",
        "\n",
        "    return body\n",
        "\n",
        "def tidyup(body):\n",
        "    \"\"\"\n",
        "    본문에서 필요없는 부분을 자르고 돌려준다.\n",
        "    :param body:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    p = body.find(TIDYUP_START_PAT)\n",
        "    # print(p)\n",
        "    body = body[:p]\n",
        "    body = body.strip()\n",
        "\n",
        "    return body\n",
        "\n",
        "def ext_body(html_text):\n",
        "    \"\"\"\n",
        "    HTML 기사에서 본문을 추출하여 돌려준다.\n",
        "    :param html_text:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    p = html_text.find(BODY_START_PAT)\n",
        "    q = html_text.find(BODY_END_PAT)\n",
        "    html_body = html_text[p + len(BODY_START_PAT):q]\n",
        "    len(html_body)\n",
        "    href_inx = html_body.find('<a')\n",
        "    # ^ M_$\n",
        "    html_body = html_body[0:href_inx-1]\n",
        "    html_body = html_body.replace(\"<br />\",\"\\n\")\n",
        "    html_body = html_body.strip()\n",
        "    body = strip_html(html_body)\n",
        "    body = tidyup(body)\n",
        "\n",
        "    return body\n",
        "\n",
        "def write_article(text_file, title, date_time, body):\n",
        "    \"\"\"\n",
        "    텍스트 파일에 항목이 구분된 기사를 출력한다.\n",
        "    :param text_file:\n",
        "    :param title:\n",
        "    :param date_time:\n",
        "    :param body:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    text_file.write(\"{}\\n\".format(title))\n",
        "    # text_file.write(\"{}\\n\".format(date_time))\n",
        "    text_file.write(\"{}\\n\".format(body))\n",
        "    # text_file.write(\"{}\\n\".format(ARTICLE_DELIMITER))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeaU37kqt_uc"
      },
      "source": [
        "target_word = \"KR모터스\" #input(\"Enter keyword : \")\n",
        "target_date = \"20180101\"\n",
        "html_file_name =  \"/content/drive/MyDrive/Colab Notebooks/stock_ML/html\"+target_word+\"__\"+target_date+\".txt\"\n",
        "text_file_name =\"/content/drive/MyDrive/Colab Notebooks/stock_ML/result\"+target_word+\"__\"+target_date+\".txt\"\n",
        "\n",
        "html_file = open(html_file_name, \"r\", encoding=\"utf-8\")\n",
        "text_file = open(text_file_name, \"w\", encoding=\"utf-8\")\n",
        "\n",
        "while True:\n",
        "\n",
        "    html_text = read_html_article(html_file)\n",
        "\n",
        "    if not html_text:\n",
        "        break\n",
        "\n",
        "    title = ext_title(html_text)\n",
        "    date_time = ext_date_time(html_text)\n",
        "    body = ext_body(html_text)\n",
        "    write_article(text_file, title, date_time, body)\n",
        "\n",
        "html_file.close()\n",
        "text_file.close()\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80gwZvqAudQF"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}